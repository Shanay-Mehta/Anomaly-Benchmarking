{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Importing ML/DL libraries\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_fscore_support, auc\n",
    "\n",
    "from tensorflow.keras  import initializers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import tensorflow as tf\n",
    "import random\n",
    "random.seed(42)\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "df = pd.read_csv('yahoo7.csv')\n",
    "dimension=len(df.columns)\n",
    "alpha_values = list()\n",
    "class PEF(Model):\n",
    "    def __init__(self):\n",
    "        super(PEF, self).__init__()\n",
    "        self.beta = tf.Variable(0.1)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        ef = x / (1 + tf.abs(x))\n",
    "        return ef * self.beta\n",
    "\n",
    "def get_generator(optimizer):\n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(64, input_dim=dimension, kernel_initializer=initializers.glorot_normal(seed=42)))\n",
    "    generator.add(PEF())\n",
    "\n",
    "    generator.add(Dense(128))\n",
    "    generator.add(PEF())\n",
    "\n",
    "    generator.add(Dense(256))\n",
    "    generator.add(PEF())\n",
    "\n",
    "    generator.add(Dense(256))\n",
    "    generator.add(PEF())\n",
    "\n",
    "    generator.add(Dense(512))\n",
    "    generator.add(PEF())\n",
    "\n",
    "    generator.add(Dense(dimension))\n",
    "    generator.add(PEF())\n",
    "\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return generator\n",
    "\n",
    "def get_discriminator(optimizer):\n",
    "    discriminator = Sequential()\n",
    "\n",
    "    discriminator.add(Dense(256, input_dim=dimension, kernel_initializer=initializers.glorot_normal(seed=42)))\n",
    "    discriminator.add(PEF())\n",
    "    discriminator.add(Dropout(0.2))\n",
    "\n",
    "    discriminator.add(Dense(128))\n",
    "    discriminator.add(PEF())\n",
    "    discriminator.add(Dropout(0.2))\n",
    "\n",
    "    discriminator.add(Dense(128))\n",
    "    discriminator.add(PEF())\n",
    "    discriminator.add(Dropout(0.2))\n",
    "\n",
    "    discriminator.add(Dense(128))\n",
    "    discriminator.add(PEF())\n",
    "    discriminator.add(Dropout(0.2))\n",
    "\n",
    "    discriminator.add(Dense(128))\n",
    "    discriminator.add(PEF())\n",
    "    discriminator.add(Dropout(0.2))\n",
    "\n",
    "    discriminator.add(Dense(dimension))\n",
    "    discriminator.add(PEF())\n",
    "\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return discriminator\n",
    "\n",
    "def get_gan_network(discriminator, generator, optimizer, input_dim=dimension):\n",
    "    discriminator.trainable = False\n",
    "    gan_input = Input(shape=(input_dim,))\n",
    "    print('input', gan_input)\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "\n",
    "    gan = Model(inputs=gan_input, outputs=gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return gan\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    learning_rate = 0.00001\n",
    "    batch_size = 150\n",
    "    epochs = 50\n",
    "    adam = Adam(lr=learning_rate, beta_1=0.5)\n",
    "    actual = pd.read_csv('yahoo7.csv', usecols=['Class']).values.ravel()\n",
    "    dataset = pd.read_csv('yahoo7.csv', usecols=lambda column: column != 'Class')\n",
    "    dimension = len(dataset.columns)\n",
    "    print(dataset)\n",
    "    dataset = dataset.astype('float32')\n",
    "    dataset = np.array(dataset)\n",
    "    # normalize the dataset\n",
    "    print('dataset', dataset)\n",
    "    test_data=dataset\n",
    "    train_size = int(len(dataset) * 0.7)\n",
    "    test_size = len(dataset)\n",
    "    train_data=[]\n",
    "    for i in range(train_size):\n",
    "        train_data.append(dataset[i])\n",
    "    test_data=dataset\n",
    "    train_data = np.array(train_data)\n",
    "    # Calculating the number of batches based on the batch size\n",
    "    batch_count = train_data.shape[0] // batch_size\n",
    "    pbar = tqdm(total=epochs * batch_count)\n",
    "    gan_loss = []\n",
    "    discriminator_loss = []\n",
    "\n",
    "    # Inititalizing the network\n",
    "    generator = get_generator(adam)\n",
    "    discriminator = get_discriminator(adam)\n",
    "    gan = get_gan_network(discriminator, generator, adam, input_dim=dimension)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for index in range(batch_count):\n",
    "            pbar.update(1)\n",
    "            # Creating a random set of input noise and images\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, dimension])\n",
    "\n",
    "            # Generate fake samples\n",
    "            generated_images = generator.predict_on_batch(noise)\n",
    "\n",
    "            # Obtain a batch of normal network packets\n",
    "            image_batch = train_data[index * batch_size: (index + 1) * batch_size]\n",
    "\n",
    "            X = np.vstack((generated_images, image_batch))\n",
    "            y_dis = np.ones(2 * batch_size)\n",
    "            y_dis[:batch_size] = 0\n",
    "\n",
    "            # Train discriminator\n",
    "            discriminator.trainable = True\n",
    "            d_loss = discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "            # Train generator\n",
    "            noise = np.random.uniform(0, 1, size=[batch_size, dimension])\n",
    "            y_gen = np.ones(batch_size)\n",
    "            discriminator.trainable = False\n",
    "            g_loss = gan.train_on_batch(noise, y_gen)\n",
    "\n",
    "            # Record the losses\n",
    "            discriminator_loss.append(d_loss)\n",
    "            gan_loss.append(g_loss)\n",
    "\n",
    "    print(\"Epoch %d Batch %d/%d [D loss: %f] [G loss:%f]\" % (epoch, index, batch_count, d_loss, g_loss))\n",
    "    dataframe1 = pd.read_csv('yahoo7.csv', usecols=['Class'], engine='python')\n",
    "    dataset1 =pd.read_csv('yahoo7.csv',usecols=lambda column: column != 'Class')\n",
    "    test_x_predictions = discriminator.predict(dataset1)\n",
    "    y_pred = np.array(test_x_predictions)\n",
    "    per = np.percentile(test_x_predictions, 10)\n",
    "    inds = (y_pred > per)\n",
    "\n",
    "    inds_comp = (y_pred <= per)\n",
    "    y_pred[inds] = 0\n",
    "    y_pred[inds_comp] = 1\n",
    "    precision_gan, recall_gan, f1_score_gan, _ = precision_recall_fscore_support(actual, y_pred.flatten(), average='binary')\n",
    "    print('Precision:', precision_gan)\n",
    "    print('Recall:', recall_gan)\n",
    "    print('F1 Score:', f1_score_gan)\n",
    "\n",
    "    # Calculate ROC curve\n",
    "    fpr6, tpr6, _ = roc_curve(actual, y_pred.flatten())\n",
    "\n",
    "    # # Calculate AUC (Area Under the ROC Curve)\n",
    "    auc_roc_gan = auc(fpr6, tpr6)\n",
    "\n",
    "    print('AUC ROC: ',auc_roc_gan)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
