{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFEjSDa6ZqutTPX1O3OhWq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptNJcCR9Gsfv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
        "\n",
        "RANDOM_SEED = 2021\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data=pd.read_csv('2_annthyroid.csv')\n",
        "    X_data = data.iloc[:, :-1].values  # Use .values to convert to a NumPy array\n",
        "    y = data.iloc[:, -1].values\n",
        "\n",
        "    X_train=[]\n",
        "\n",
        "    normal_data = X_data[y == 0]\n",
        "\n",
        "    train_threshold = math.floor(0.7 * len(data))\n",
        "    for i in range(train_threshold):\n",
        "        X_train.append(normal_data[i])\n",
        "\n",
        "    X = np.array(X_data)\n",
        "    min_val = tf.reduce_min(X)\n",
        "    max_val = tf.reduce_max(X)\n",
        "    X = (X - min_val) / (max_val - min_val)\n",
        "    X = tf.cast(X, tf.float32)\n",
        "\n",
        "    nb_epoch = 50\n",
        "    batch_size = 64\n",
        "    input_dim = X.shape[1]  # num of columns\n",
        "    encoding_dim = 14\n",
        "    hidden_dim_1 = int(encoding_dim / 2)\n",
        "    hidden_dim_2 = 4\n",
        "    learning_rate = 1e-7\n",
        "\n",
        "    input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
        "    encoder = tf.keras.layers.Dense(encoding_dim, activation=\"tanh\",\n",
        "                                    activity_regularizer=tf.keras.regularizers.l2(learning_rate))(input_layer)\n",
        "    encoder = tf.keras.layers.Dropout(0.2)(encoder)\n",
        "    encoder = tf.keras.layers.Dense(hidden_dim_1, activation='relu')(encoder)\n",
        "    encoder = tf.keras.layers.Dense(hidden_dim_2, activation=tf.nn.leaky_relu)(encoder)\n",
        "\n",
        "    # Decoder\n",
        "    decoder = tf.keras.layers.Dense(hidden_dim_1, activation='relu')(encoder)\n",
        "    decoder = tf.keras.layers.Dropout(0.2)(decoder)\n",
        "    decoder = tf.keras.layers.Dense(encoding_dim, activation='relu')(decoder)\n",
        "    decoder = tf.keras.layers.Dense(input_dim, activation='tanh')(decoder)\n",
        "\n",
        "    autoencoder = tf.keras.Model(inputs=input_layer, outputs=decoder)\n",
        "    autoencoder.summary()\n",
        "\n",
        "    cp = tf.keras.callbacks.ModelCheckpoint(filepath=\"autoencoder_fraud.h5\",\n",
        "                                            mode='min', monitor='val_loss', verbose=2, save_best_only=True)\n",
        "\n",
        "    # Define early stopping\n",
        "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        min_delta=0.0001,\n",
        "        patience=10,\n",
        "        verbose=1,\n",
        "        mode='min',\n",
        "        restore_best_weights=True)\n",
        "\n",
        "    autoencoder.compile(metrics=['accuracy'],\n",
        "                        loss='mean_squared_error',\n",
        "                        optimizer='adam')\n",
        "\n",
        "\n",
        "    X_test=X_data\n",
        "    X_train = np.array(X_train)\n",
        "\n",
        "\n",
        "    history = autoencoder.fit(X_train, X_train,\n",
        "                              epochs=nb_epoch,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True,\n",
        "                              validation_data=(X_test, X_test),\n",
        "                              verbose=1,\n",
        "                              callbacks=[cp, early_stop]\n",
        "                              ).history\n",
        "\n",
        "    test_x_predictions = autoencoder.predict(X_test)\n",
        "    mse = np.mean(np.power(X_test - test_x_predictions, 2), axis=1)\n",
        "\n",
        "    y_probs = mse  # You can use MSE as your anomaly score\n",
        "\n",
        "    lper = np.percentile(mse, 5)\n",
        "    uper = np.percentile(mse, 95)\n",
        "\n",
        "    # Classify data points as anomalies (1) or normal (0)\n",
        "    y_pred = (mse <= lper) | (mse >= uper)\n",
        "\n",
        "    # Calculate precision, recall, F1 score, and AUC-ROC\n",
        "    precision_auto = precision_score(y, y_pred)\n",
        "    recall_auto = recall_score(y, y_pred)\n",
        "    f1_score_auto = f1_score(y, y_pred)\n",
        "    fpr8, tpr8, thresholds = roc_curve(y, y_pred)\n",
        "    auc_roc_auto = auc(fpr8, tpr8)\n",
        "\n",
        "\n",
        "    print(\"Precision: {:.4f}\".format(precision_auto))\n",
        "    print(\"Recall: {:.4f}\".format(recall_auto))\n",
        "    print(\"F1-score: {:.4f}\".format(f1_score_auto))\n",
        "    print(\"AUC-ROC: {:.4f}\".format(auc_roc_auto))"
      ]
    }
  ]
}