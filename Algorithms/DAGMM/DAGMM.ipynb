{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQ4nw2MA6fz5290e9Morbn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "class CompressionNet:\n",
        "    \"\"\" Compression Network.\n",
        "    This network converts the input data to the representations\n",
        "    suitable for calculation of anormaly scores by \"Estimation Network\".\n",
        "\n",
        "    Outputs of network consist of next 2 components:\n",
        "    1) reduced low-dimensional representations learned by AutoEncoder.\n",
        "    2) the features derived from reconstruction error.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_layer_sizes, activation=tf.nn.tanh):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        hidden_layer_sizes : list of int\n",
        "            list of the size of hidden layers.\n",
        "            For example, if the sizes are [n1, n2],\n",
        "            the sizes of created networks are:\n",
        "            input_size -> n1 -> n2 -> n1 -> input_sizes\n",
        "            (network outputs the representation of \"n2\" layer)\n",
        "        activation : function\n",
        "            activation function of hidden layer.\n",
        "            the last layer uses linear function.\n",
        "        \"\"\"\n",
        "        self.hidden_layer_sizes = hidden_layer_sizes\n",
        "        self.activation = activation\n",
        "\n",
        "    def compress(self, x):\n",
        "        self.input_size = x.shape[1]\n",
        "\n",
        "        with tf.variable_scope(\"Encoder\"):\n",
        "            z = x\n",
        "            n_layer = 0\n",
        "            for size in self.hidden_layer_sizes[:-1]:\n",
        "                n_layer += 1\n",
        "                z = tf.layers.dense(z, size, activation=self.activation,\n",
        "                    name=\"layer_{}\".format(n_layer))\n",
        "\n",
        "            # activation function of last layer is linear\n",
        "            n_layer += 1\n",
        "            z = tf.layers.dense(z, self.hidden_layer_sizes[-1],\n",
        "                name=\"layer_{}\".format(n_layer))\n",
        "\n",
        "        return z\n",
        "\n",
        "    def reverse(self, z):\n",
        "        with tf.variable_scope(\"Decoder\"):\n",
        "            n_layer = 0\n",
        "            for size in self.hidden_layer_sizes[:-1][::-1]:\n",
        "                n_layer += 1\n",
        "                z = tf.layers.dense(z, size, activation=self.activation,\n",
        "                    name=\"layer_{}\".format(n_layer))\n",
        "\n",
        "            # activation function of last layes is linear\n",
        "            n_layer += 1\n",
        "            x_dash = tf.layers.dense(z, self.input_size,\n",
        "                name=\"layer_{}\".format(n_layer))\n",
        "\n",
        "        return x_dash\n",
        "\n",
        "    def loss(self, x, x_dash):\n",
        "        def euclid_norm(x):\n",
        "            return tf.sqrt(tf.reduce_sum(tf.square(x), axis=1))\n",
        "\n",
        "        # Calculate Euclid norm, distance\n",
        "        norm_x = euclid_norm(x)\n",
        "        norm_x_dash = euclid_norm(x_dash)\n",
        "        dist_x = euclid_norm(x - x_dash)\n",
        "        dot_x = tf.reduce_sum(x * x_dash, axis=1)\n",
        "\n",
        "        # Based on the original paper, features of reconstraction error\n",
        "        # are composed of these loss functions:\n",
        "        #  1. loss_E : relative Euclidean distance\n",
        "        #  2. loss_C : cosine similarity\n",
        "        min_val = 1e-3\n",
        "        loss_E = dist_x  / (norm_x + min_val)\n",
        "        loss_C = 0.5 * (1.0 - dot_x / (norm_x * norm_x_dash + min_val))\n",
        "        return tf.concat([loss_E[:,None], loss_C[:,None]], axis=1)\n",
        "\n",
        "    def extract_feature(self, x, x_dash, z_c):\n",
        "        z_r = self.loss(x, x_dash)\n",
        "        return tf.concat([z_c, z_r], axis=1)\n",
        "\n",
        "    def inference(self, x):\n",
        "        \"\"\" convert input to output tensor, which is composed of\n",
        "        low-dimensional representation and reconstruction error.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : tf.Tensor shape : (n_samples, n_features)\n",
        "            Input data\n",
        "\n",
        "        Results\n",
        "        -------\n",
        "        z : tf.Tensor shape : (n_samples, n2 + 2)\n",
        "            Result data\n",
        "            Second dimension of this data is equal to\n",
        "            sum of compressed representation size and\n",
        "            number of loss function (=2)\n",
        "\n",
        "        x_dash : tf.Tensor shape : (n_samples, n_features)\n",
        "            Reconstructed data for calculation of\n",
        "            reconstruction error.\n",
        "        \"\"\"\n",
        "\n",
        "        with tf.variable_scope(\"CompNet\"):\n",
        "            # AutoEncoder\n",
        "            z_c = self.compress(x)\n",
        "            x_dash = self.reverse(z_c)\n",
        "\n",
        "            # compose feature vector\n",
        "            z = self.extract_feature(x, x_dash, z_c)\n",
        "\n",
        "        return z, x_dash\n",
        "\n",
        "    def reconstruction_error(self, x, x_dash):\n",
        "        return tf.reduce_mean(tf.reduce_sum(\n",
        "            tf.square(x - x_dash), axis=1), axis=0)\n"
      ],
      "metadata": {
        "id": "rvHrjEONzIMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "class EstimationNet:\n",
        "    \"\"\" Estimation Network\n",
        "\n",
        "    This network converts input feature vector to softmax probability.\n",
        "    Bacause loss function for this network is not defined,\n",
        "    it should be implemented outside of this class.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_layer_sizes, activation=tf.nn.relu):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        hidden_layer_sizes : list of int\n",
        "            list of sizes of hidden layers.\n",
        "            For example, if the sizes are [n1, n2],\n",
        "            layer sizes of the network are:\n",
        "            input_size -> n1 -> n2\n",
        "            (network outputs the softmax probabilities of \"n2\" layer)\n",
        "        activation : function\n",
        "            activation function of hidden layer.\n",
        "            the funtcion of last layer is softmax function.\n",
        "        \"\"\"\n",
        "        self.hidden_layer_sizes = hidden_layer_sizes\n",
        "        self.activation = activation\n",
        "\n",
        "    def inference(self, z, dropout_ratio=None):\n",
        "        \"\"\" Output softmax probabilities\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        z : tf.Tensor shape : (n_samples, n_features)\n",
        "            Data inferenced by this network\n",
        "        dropout_ratio : tf.Tensor shape : 0-dimension float (optional)\n",
        "            Specify dropout ratio\n",
        "            (if None, dropout is not applied)\n",
        "\n",
        "        Results\n",
        "        -------\n",
        "        probs : tf.Tensor shape : (n_samples, n_classes)\n",
        "            Calculated probabilities\n",
        "        \"\"\"\n",
        "        with tf.variable_scope(\"EstNet\"):\n",
        "            n_layer = 0\n",
        "            for size in self.hidden_layer_sizes[:-1]:\n",
        "                n_layer += 1\n",
        "                z = tf.layers.dense(z, size, activation=self.activation,\n",
        "                    name=\"layer_{}\".format(n_layer))\n",
        "                if dropout_ratio is not None:\n",
        "                    z = tf.layers.dropout(z, dropout_ratio,\n",
        "                        name=\"drop_{}\".format(n_layer))\n",
        "\n",
        "            # Last layer uses linear function (=logits)\n",
        "            size = self.hidden_layer_sizes[-1]\n",
        "            logits = tf.layers.dense(z, size, activation=None, name=\"logits\")\n",
        "\n",
        "            # Softmax output\n",
        "            output = tf.nn.softmax(logits)\n",
        "\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "3zFb1pHd1ZP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "class GMM:\n",
        "    \"\"\" Gaussian Mixture Model (GMM) \"\"\"\n",
        "    def __init__(self, n_comp):\n",
        "        self.n_comp = n_comp\n",
        "        self.phi = self.mu = self.sigma = None\n",
        "        self.training = False\n",
        "\n",
        "    def create_variables(self, n_features):\n",
        "        with tf.variable_scope(\"GMM\"):\n",
        "            phi = tf.Variable(tf.zeros(shape=[self.n_comp]),\n",
        "                dtype=tf.float32, name=\"phi\")\n",
        "            mu = tf.Variable(tf.zeros(shape=[self.n_comp, n_features]),\n",
        "                dtype=tf.float32, name=\"mu\")\n",
        "            sigma = tf.Variable(tf.zeros(\n",
        "                shape=[self.n_comp, n_features, n_features]),\n",
        "                dtype=tf.float32, name=\"sigma\")\n",
        "            L = tf.Variable(tf.zeros(\n",
        "                shape=[self.n_comp, n_features, n_features]),\n",
        "                dtype=tf.float32, name=\"L\")\n",
        "\n",
        "        return phi, mu, sigma, L\n",
        "\n",
        "    def fit(self, z, gamma):\n",
        "        \"\"\" fit data to GMM model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        z : tf.Tensor, shape (n_samples, n_features)\n",
        "            data fitted to GMM.\n",
        "        gamma : tf.Tensor, shape (n_samples, n_comp)\n",
        "            probability. each row is correspond to row of z.\n",
        "        \"\"\"\n",
        "\n",
        "        with tf.variable_scope(\"GMM\"):\n",
        "            # Calculate mu, sigma\n",
        "            # i   : index of samples\n",
        "            # k   : index of components\n",
        "            # l,m : index of features\n",
        "            gamma_sum = tf.reduce_sum(gamma, axis=0)\n",
        "            self.phi = phi = tf.reduce_mean(gamma, axis=0)\n",
        "            self.mu = mu = tf.einsum('ik,il->kl', gamma, z) / gamma_sum[:,None]\n",
        "            z_centered = tf.sqrt(gamma[:,:,None]) * (z[:,None,:] - mu[None,:,:])\n",
        "            self.sigma = sigma = tf.einsum(\n",
        "                'ikl,ikm->klm', z_centered, z_centered) / gamma_sum[:,None,None]\n",
        "\n",
        "            # Calculate a cholesky decomposition of covariance in advance\n",
        "            n_features = z.shape[1]\n",
        "            min_vals = tf.diag(tf.ones(n_features, dtype=tf.float32)) * 1e-6\n",
        "            self.L = tf.cholesky(sigma + min_vals[None,:,:])\n",
        "\n",
        "        self.training = False\n",
        "\n",
        "    def fix_op(self):\n",
        "        \"\"\" return operator to fix paramters of GMM\n",
        "        Using this operator outside of this class,\n",
        "        you can fix current parameter to static tensor variable.\n",
        "\n",
        "        After you call this method, you have to run result\n",
        "        operator immediatelly, and call energy() to use static\n",
        "        variables of model parameter.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        op : operator of tensorflow\n",
        "            operator to assign current parameter to variables\n",
        "        \"\"\"\n",
        "\n",
        "        phi, mu, sigma, L = self.create_variables(self.mu.shape[1])\n",
        "\n",
        "        op = tf.group(\n",
        "            tf.assign(phi, self.phi),\n",
        "            tf.assign(mu, self.mu),\n",
        "            tf.assign(sigma, self.sigma),\n",
        "            tf.assign(L, self.L)\n",
        "        )\n",
        "\n",
        "        self.phi, self.phi_org = phi, self.phi\n",
        "        self.mu, self.mu_org = mu, self.mu\n",
        "        self.sigma, self.sigma_org = sigma, self.sigma\n",
        "        self.L, self.L_org = L, self.L\n",
        "\n",
        "        self.training = False\n",
        "\n",
        "        return op\n",
        "\n",
        "    def energy(self, z):\n",
        "        \"\"\" calculate an energy of each row of z\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        z : tf.Tensor, shape (n_samples, n_features)\n",
        "            data each row of which is calculated its energy.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        energy : tf.Tensor, shape (n_samples)\n",
        "            calculated energies\n",
        "        \"\"\"\n",
        "\n",
        "        if self.training and self.phi is None:\n",
        "            self.phi, self.mu, self.sigma, self.L = self.create_variable(z.shape[1])\n",
        "\n",
        "        with tf.variable_scope(\"GMM_energy\"):\n",
        "            # Instead of inverse covariance matrix, exploit cholesky decomposition\n",
        "            # for stability of calculation.\n",
        "            z_centered = z[:,None,:] - self.mu[None,:,:]  #ikl\n",
        "            v = tf.matrix_triangular_solve(self.L, tf.transpose(z_centered, [1, 2, 0]))  # kli\n",
        "\n",
        "            # log(det(Sigma)) = 2 * sum[log(diag(L))]\n",
        "            log_det_sigma = 2.0 * tf.reduce_sum(tf.log(tf.matrix_diag_part(self.L)), axis=1)\n",
        "\n",
        "            # To calculate energies, use \"log-sum-exp\" (different from orginal paper)\n",
        "            d = z.get_shape().as_list()[1]\n",
        "            logits = tf.log(self.phi[:,None]) - 0.5 * (tf.reduce_sum(tf.square(v), axis=1)\n",
        "                + d * tf.log(2.0 * np.pi) + log_det_sigma[:,None])\n",
        "            energies = - tf.reduce_logsumexp(logits, axis=0)\n",
        "\n",
        "        return energies\n",
        "\n",
        "    def cov_diag_loss(self):\n",
        "        with tf.variable_scope(\"GMM_diag_loss\"):\n",
        "            diag_loss = tf.reduce_sum(tf.divide(1, tf.matrix_diag_part(self.sigma)))\n",
        "\n",
        "        return diag_loss\n"
      ],
      "metadata": {
        "id": "vDXm2kbb1dUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "from os import makedirs\n",
        "from os.path import exists, join\n",
        "\n",
        "class DAGMM:\n",
        "    \"\"\" Deep Autoencoding Gaussian Mixture Model.\n",
        "\n",
        "    This implementation is based on the paper:\n",
        "    Bo Zong+ (2018) Deep Autoencoding Gaussian Mixture Model\n",
        "    for Unsupervised Anomaly Detection, ICLR 2018\n",
        "    (this is UNOFFICIAL implementation)\n",
        "    \"\"\"\n",
        "\n",
        "    MODEL_FILENAME = \"DAGMM_model\"\n",
        "    SCALER_FILENAME = \"DAGMM_scaler\"\n",
        "\n",
        "    def __init__(self, comp_hiddens, comp_activation,\n",
        "            est_hiddens, est_activation, est_dropout_ratio=0.5,\n",
        "            minibatch_size=1024, epoch_size=100,\n",
        "            learning_rate=0.0001, lambda1=0.1, lambda2=0.0001,\n",
        "            normalize=True, random_seed=123):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        comp_hiddens : list of int\n",
        "            sizes of hidden layers of compression network\n",
        "            For example, if the sizes are [n1, n2],\n",
        "            structure of compression network is:\n",
        "            input_size -> n1 -> n2 -> n1 -> input_sizes\n",
        "        comp_activation : function\n",
        "            activation function of compression network\n",
        "        est_hiddens : list of int\n",
        "            sizes of hidden layers of estimation network.\n",
        "            The last element of this list is assigned as n_comp.\n",
        "            For example, if the sizes are [n1, n2],\n",
        "            structure of estimation network is:\n",
        "            input_size -> n1 -> n2 (= n_comp)\n",
        "        est_activation : function\n",
        "            activation function of estimation network\n",
        "        est_dropout_ratio : float (optional)\n",
        "            dropout ratio of estimation network applied during training\n",
        "            if 0 or None, dropout is not applied.\n",
        "        minibatch_size: int (optional)\n",
        "            mini batch size during training\n",
        "        epoch_size : int (optional)\n",
        "            epoch size during training\n",
        "        learning_rate : float (optional)\n",
        "            learning rate during training\n",
        "        lambda1 : float (optional)\n",
        "            a parameter of loss function (for energy term)\n",
        "        lambda2 : float (optional)\n",
        "            a parameter of loss function\n",
        "            (for sum of diagonal elements of covariance)\n",
        "        normalize : bool (optional)\n",
        "            specify whether input data need to be normalized.\n",
        "            by default, input data is normalized.\n",
        "        random_seed : int (optional)\n",
        "            random seed used when fit() is called.\n",
        "        \"\"\"\n",
        "        self.comp_net = CompressionNet(comp_hiddens, comp_activation)\n",
        "        self.est_net = EstimationNet(est_hiddens, est_activation)\n",
        "        self.est_dropout_ratio = est_dropout_ratio\n",
        "\n",
        "        n_comp = est_hiddens[-1]\n",
        "        self.gmm = GMM(n_comp)\n",
        "\n",
        "        self.minibatch_size = minibatch_size\n",
        "        self.epoch_size = epoch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.lambda1 = lambda1\n",
        "        self.lambda2 = lambda2\n",
        "\n",
        "        self.normalize = normalize\n",
        "        self.scaler = None\n",
        "        self.seed = random_seed\n",
        "\n",
        "        self.graph = None\n",
        "        self.sess = None\n",
        "\n",
        "    def __del__(self):\n",
        "        if self.sess is not None:\n",
        "            self.sess.close()\n",
        "\n",
        "    def fit(self, x):\n",
        "        \"\"\" Fit the DAGMM model according to the given data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array-like, shape (n_samples, n_features)\n",
        "            Training data.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = x.shape\n",
        "\n",
        "        if self.normalize:\n",
        "            self.scaler = scaler = StandardScaler()\n",
        "            x = scaler.fit_transform(x)\n",
        "\n",
        "        with tf.Graph().as_default() as graph:\n",
        "            self.graph = graph\n",
        "            tf.set_random_seed(self.seed)\n",
        "            np.random.seed(seed=self.seed)\n",
        "\n",
        "            # Create Placeholder\n",
        "            self.input = input = tf.placeholder(\n",
        "                dtype=tf.float32, shape=[None, n_features])\n",
        "            self.drop = drop = tf.placeholder(dtype=tf.float32, shape=[])\n",
        "\n",
        "            # Build graph\n",
        "            z, x_dash  = self.comp_net.inference(input)\n",
        "            gamma = self.est_net.inference(z, drop)\n",
        "            self.gmm.fit(z, gamma)\n",
        "            energy = self.gmm.energy(z)\n",
        "\n",
        "            self.x_dash = x_dash\n",
        "\n",
        "            # Loss function\n",
        "            loss = (self.comp_net.reconstruction_error(input, x_dash) +\n",
        "                self.lambda1 * tf.reduce_mean(energy) +\n",
        "                self.lambda2 * self.gmm.cov_diag_loss())\n",
        "\n",
        "            # Minimizer\n",
        "            minimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(loss)\n",
        "\n",
        "            # Number of batch\n",
        "            n_batch = (n_samples - 1) // self.minibatch_size + 1\n",
        "\n",
        "            # Create tensorflow session and initilize\n",
        "            init = tf.global_variables_initializer()\n",
        "\n",
        "            self.sess = tf.Session(graph=graph)\n",
        "            self.sess.run(init)\n",
        "\n",
        "            # Training\n",
        "            idx = np.arange(x.shape[0])\n",
        "            np.random.shuffle(idx)\n",
        "\n",
        "            for epoch in range(self.epoch_size):\n",
        "                for batch in range(n_batch):\n",
        "                    i_start = batch * self.minibatch_size\n",
        "                    i_end = (batch + 1) * self.minibatch_size\n",
        "                    x_batch = x[idx[i_start:i_end]]\n",
        "\n",
        "                    self.sess.run(minimizer, feed_dict={\n",
        "                        input:x_batch, drop:self.est_dropout_ratio})\n",
        "\n",
        "                if (epoch + 1) % 100 == 0:\n",
        "                    loss_val = self.sess.run(loss, feed_dict={input:x, drop:0})\n",
        "                    print(\" epoch {}/{} : loss = {:.3f}\".format(epoch + 1, self.epoch_size, loss_val))\n",
        "\n",
        "            # Fix GMM parameter\n",
        "            fix = self.gmm.fix_op()\n",
        "            self.sess.run(fix, feed_dict={input:x, drop:0})\n",
        "            self.energy = self.gmm.energy(z)\n",
        "\n",
        "            tf.add_to_collection(\"save\", self.input)\n",
        "            tf.add_to_collection(\"save\", self.energy)\n",
        "\n",
        "            self.saver = tf.train.Saver()\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\" Calculate anormaly scores (sample energy) on samples in X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array-like, shape (n_samples, n_features)\n",
        "            Data for which anomaly scores are calculated.\n",
        "            n_features must be equal to n_features of the fitted data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        energies : array-like, shape (n_samples)\n",
        "            Calculated sample energies.\n",
        "        \"\"\"\n",
        "        if self.sess is None:\n",
        "            raise Exception(\"Trained model does not exist.\")\n",
        "\n",
        "        if self.normalize:\n",
        "            x = self.scaler.transform(x)\n",
        "\n",
        "        energies = self.sess.run(self.energy, feed_dict={self.input:x})\n",
        "        return energies\n",
        "\n",
        "    def save(self, fdir):\n",
        "        \"\"\" Save trained model to designated directory.\n",
        "        This method have to be called after training.\n",
        "        (If not, throw an exception)\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        fdir : str\n",
        "            Path of directory trained model is saved.\n",
        "            If not exists, it is created automatically.\n",
        "        \"\"\"\n",
        "        if self.sess is None:\n",
        "            raise Exception(\"Trained model does not exist.\")\n",
        "\n",
        "        if not exists(fdir):\n",
        "            makedirs(fdir)\n",
        "\n",
        "        model_path = join(fdir, self.MODEL_FILENAME)\n",
        "        self.saver.save(self.sess, model_path)\n",
        "\n",
        "        if self.normalize:\n",
        "            scaler_path = join(fdir, self.SCALER_FILENAME)\n",
        "            joblib.dump(self.scaler, scaler_path)\n",
        "\n",
        "    def restore(self, fdir):\n",
        "        \"\"\" Restore trained model from designated directory.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        fdir : str\n",
        "            Path of directory trained model is saved.\n",
        "        \"\"\"\n",
        "        if not exists(fdir):\n",
        "            raise Exception(\"Model directory does not exist.\")\n",
        "\n",
        "        model_path = join(fdir, self.MODEL_FILENAME)\n",
        "        meta_path = model_path + \".meta\"\n",
        "\n",
        "        with tf.Graph().as_default() as graph:\n",
        "            self.graph = graph\n",
        "            self.sess = tf.Session(graph=graph)\n",
        "            self.saver = tf.train.import_meta_graph(meta_path)\n",
        "            self.saver.restore(self.sess, model_path)\n",
        "\n",
        "            self.input, self.energy = tf.get_collection(\"save\")\n",
        "\n",
        "        if self.normalize:\n",
        "            scaler_path = join(fdir, self.SCALER_FILENAME)\n",
        "            self.scaler = joblib.load(scaler_path)\n"
      ],
      "metadata": {
        "id": "BDdYAKGh1fGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "data = pd.read_csv('2_annthyroid.csv')\n",
        "\n",
        "X = data.iloc[:, :-1].values  # Use .values to convert to a NumPy array\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "X_train=[]\n",
        "\n",
        "normal_data = X[y == 0]\n",
        "\n",
        "train_threshold = math.floor(0.7 * len(data))\n",
        "for i in range(train_threshold):\n",
        "    X_train.append(normal_data[i])\n",
        "X_test=X"
      ],
      "metadata": {
        "id": "s7zQpK1w1xZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    X_train = pd.DataFrame(X_train)\n",
        "\n",
        "    model = DAGMM(\n",
        "        comp_hiddens=[60, 30, 10, 1], comp_activation=tf.nn.tanh,\n",
        "        est_hiddens=[10, 4], est_dropout_ratio=0.5, est_activation=tf.nn.tanh,\n",
        "        learning_rate=0.0001, epoch_size=200, minibatch_size=1024, random_seed=1111\n",
        "    )\n",
        "    model.fit(X_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Energy threshold to detect anomaly = 90th percentile of energies\n",
        "    anomaly_energy_threshold = np.percentile(y_pred, 90)\n",
        "    print(f\"Energy threshold to detect anomaly: {anomaly_energy_threshold:.3f}\")\n",
        "\n",
        "    # Detect anomalies from test data\n",
        "    y_pred_flag = np.where(y_pred >= anomaly_energy_threshold, 1, 0)\n",
        "\n",
        "    mean_mse = np.mean(y_pred)\n",
        "    std_mse = np.std(y_pred)\n",
        "    high_threshold = mean_mse + 2 * std_mse\n",
        "    low_threshold = mean_mse - 2 * std_mse\n",
        "\n",
        "    precision_dagmm, recall_dagmm, f1_score_dagmm, _ = precision_recall_fscore_support(y, y_pred_flag, average=\"binary\")\n",
        "    print(f\" Precision = {precision_dagmm:.4f}\")\n",
        "    print(f\" Recall    = {recall_dagmm:.4f}\")\n",
        "    print(f\" F1-Score  = {f1_score_dagmm:.4f}\")\n",
        "\n",
        "    # Calculate ROC curve\n",
        "    fpr5, tpr5, _ = roc_curve(y, y_pred_flag)\n",
        "\n",
        "    # Calculate AUC (Area Under the ROC Curve)\n",
        "    auc_roc_dagmm = auc(fpr5, tpr5)\n",
        "\n",
        "    # Print the AUC-ROC value\n",
        "    print(\"AUC-ROC: {:.4f}\".format(auc_roc_dagmm))"
      ],
      "metadata": {
        "id": "hGdZL1LF1g1W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}