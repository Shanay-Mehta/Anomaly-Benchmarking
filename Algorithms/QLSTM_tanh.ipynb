{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from keract import get_activations, persist_to_json_file, load_activations_from_json_file\n",
    "period_size = 76\n",
    "step_size = 38\n",
    "upper_q_threshold=0.5\n",
    "lower_q_threshold=0.0001\n",
    "output = 'activations.json'\n",
    "path='final.csv'\n",
    "first_layer=[]\n",
    "second_layer=[]\n",
    "third_layer=[]\n",
    "fourth_layer=[]\n",
    "class LogThirdLayerOutput(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        outputs = []\n",
    "        x = numpy.ones((1,2, 1))\n",
    "        activations = get_activations(self.model, x)\n",
    "        row=''\n",
    "        for k, v in activations.items():\n",
    "            print('key',k,'value', v.tolist())\n",
    "            if k=='lstm':\n",
    "                lstm_array = v.tolist()\n",
    "                lstm_array = lstm_array[0]\n",
    "                for i in range(len(lstm_array)):\n",
    "                    row = row + \",\" + str(lstm_array[i])\n",
    "                    if i == 0:\n",
    "                        first_layer.append(lstm_array[i])\n",
    "                    elif i == 1:\n",
    "                        second_layer.append(lstm_array[i])\n",
    "                    elif i == 2:\n",
    "                        third_layer.append(lstm_array[i])\n",
    "                    else:\n",
    "                        fourth_layer.append(lstm_array[i])\n",
    "        row=row+\"\\n\"\n",
    "        csv_file = open(path, 'a')\n",
    "        csv_file.write(row)\n",
    "\n",
    "        # persist the activations to the disk.\n",
    "\n",
    "        persist_to_json_file(activations, output)\n",
    "\n",
    "        # read them from the disk.\n",
    "        activations2 = load_activations_from_json_file(output)\n",
    "\n",
    "        # print them.\n",
    "        print(list(activations.keys()))\n",
    "        print(list(activations2.keys()))\n",
    "        print('Dumped to {}.'.format(output))\n",
    "def intersection(lst1, lst2):\n",
    "    lst3=[]\n",
    "    for itr in range(len(lst1)):\n",
    "        if lst1[itr][0] in lst2:\n",
    "            lst3.append(lst1[itr])\n",
    "    return lst3\n",
    "\n",
    "def verify_stationarity(dataset):\n",
    "    is_stationary=True\n",
    "    test_results = adfuller(dataset)\n",
    "\n",
    "    print(f\"ADF test statistic: {test_results[0]}\")\n",
    "    print(f\"p-value: {test_results[1]}\")\n",
    "    print(\"Critical thresholds:\")\n",
    "\n",
    "    for key, value in test_results[4].items():\n",
    "        print(f\"\\t{key}: {value}\")\n",
    "    itr = 0\n",
    "    for key, value in test_results[4].items():\n",
    "       print('\\t%s: %.3f' % (key, value))\n",
    "       if itr==0:\n",
    "         critical=value\n",
    "       itr=itr+1\n",
    "\n",
    "    print('critical',critical)\n",
    "    if test_results[0] > critical:\n",
    "         print('non stationary')\n",
    "         is_stationary=False\n",
    "    return  is_stationary\n",
    "\n",
    "def create_dataset(dataset, look_back=1, tw=3):\n",
    "    dataX, dataY = [], []  # dtaset for mean\n",
    "    datastdX, datastdY = [], []  # dataset for std\n",
    "    datacombX, datacomY = [], []  # dataset for mean and std for third deep learning\n",
    "    multi = look_back // tw\n",
    "    for i in range(len(dataset) - look_back - 1):\n",
    "        q50X = []\n",
    "        a = dataset[i + 1:(i + look_back + 1)]\n",
    "        indices = i + (multi - 1) * tw\n",
    "        c = numpy.quantile(a, upper_q_threshold)\n",
    "        for j in range(0, len(a), tw):\n",
    "            q50 = numpy.quantile(a[j:j + tw], upper_q_threshold)\n",
    "            q50X.append(q50)\n",
    "        dataX.append(q50X)\n",
    "        dataY.append(c)\n",
    "\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "def identify_anomaly_quantiles(prediction_errors):\n",
    "    anomaly_detection=[]\n",
    "    for m in range(0, len(prediction_errors), period_size):\n",
    "        period_prediction_errors=prediction_errors[m:m + period_size]\n",
    "        upper_threshold = numpy.quantile(prediction_errors[m:m + period_size],0.9)\n",
    "        lower_threshold = numpy.quantile(prediction_errors[m:m + period_size],0.1)\n",
    "        for i in range(len(period_prediction_errors)):\n",
    "            if ( period_prediction_errors[i]> upper_threshold) or (period_prediction_errors[i]<0 and period_prediction_errors[i]< lower_threshold):\n",
    "                anomaly_detection.append(period_prediction_errors[i])\n",
    "\n",
    "    return anomaly_detection\n",
    "\n",
    "def identify_anomaly(prediction_errors):\n",
    "    anomaly_detection=[]\n",
    "    for m in range(0, len(prediction_errors), period_size):\n",
    "        period_prediction_errors=prediction_errors[m:m + period_size]\n",
    "        avg = numpy.average(prediction_errors[m:m + period_size])\n",
    "        std1 = numpy.std(prediction_errors[m:m + period_size])\n",
    "        upper_threshold=avg+1.6*std1\n",
    "        lower_threshold = avg - 1.6* std1\n",
    "        for i in range(len(period_prediction_errors)):\n",
    "            if (period_prediction_errors[i]> upper_threshold) or ( period_prediction_errors[i]< lower_threshold):\n",
    "                anomaly_detection.append(period_prediction_errors[i])\n",
    "\n",
    "    return  anomaly_detection\n",
    "if __name__ == '__main__':\n",
    "    # fix random seed for reproducibility\n",
    "    numpy.random.seed(7)\n",
    "    # load the dataset\n",
    "    dataframe = read_csv('yahoo3_train.csv', usecols=[0], engine='python') # yahoo3_train contains all the normal datapoints in yahoo3\n",
    "    dataset = dataframe.values\n",
    "    stationary=verify_stationarity(dataset)\n",
    "    dataset = dataset.astype('float32')\n",
    "    # normalize the dataset\n",
    "    print('dataset', dataset)\n",
    "    stationary = verify_stationarity(dataset)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    dataset = scaler.fit_transform(dataset)\n",
    "    # split into train and test sets\n",
    "    train_size = int(len(dataset) * 0.3)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\n",
    "    # reshape into X=t and Y=t+1\n",
    "    look_back = period_size\n",
    "    tw = step_size\n",
    "    multi = look_back // tw\n",
    "    trainX, trainY = create_dataset(train, look_back, tw)\n",
    "    testX, testY = create_dataset(test, look_back, tw)\n",
    "    print(trainX)\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "    testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "    print(trainX)\n",
    "\n",
    "    modelq10 = Sequential()\n",
    "    modelq10.add(LSTM(4, input_shape=(multi, 1), activation='tanh',recurrent_activation='tanh'))\n",
    "    modelq10.add(Dense(1))\n",
    "\n",
    "    modelq10.compile(loss=losses.logcosh, optimizer='adam')\n",
    "\n",
    "    modelq10.fit(trainX, trainY, epochs=50, batch_size=1, verbose=2, callbacks=[LogThirdLayerOutput()])\n",
    "\n",
    "    i = 0\n",
    "    j = look_back\n",
    "    actual_quantile_interval = []\n",
    "    steps = tw\n",
    "    positive = True\n",
    "    anomalies=[]\n",
    "    finalres_q10 = []\n",
    "    finalres_q90 = []\n",
    "    dataframe = read_csv('yahoo3.csv', usecols=[0], engine='python')\n",
    "    dataset = dataframe.values\n",
    "    dataset = scaler.fit_transform(dataset)\n",
    "    ts = dataset\n",
    "    ts_accumulate=[]\n",
    "    comparison_dataset=[]\n",
    "    while j <= len(dataset):\n",
    "        q50_array = []\n",
    "\n",
    "\n",
    "        temp = dataset[i:j]\n",
    "        actual_quantile_interval.append(\n",
    "            numpy.absolute(numpy.quantile(dataset[i + 1:j + 1], lower_q_threshold) - numpy.quantile(dataset[i + 1:j + 1], upper_q_threshold)))\n",
    "        print('print here', temp)\n",
    "\n",
    "        for m in range(0, len(temp), steps):\n",
    "            q50array = []\n",
    "            q50 = numpy.quantile(temp[m:m + steps], upper_q_threshold)\n",
    "            q50array.append(q50)\n",
    "            q50_array.append(q50array)\n",
    "\n",
    "        final_q50_array = []\n",
    "        final_q50_array.append(q50_array)\n",
    "        print('final_q10_array', final_q50_array)\n",
    "        q50_predict = modelq10.predict(final_q50_array)\n",
    "        print('q50_predict', q50_predict)\n",
    "\n",
    "        if j+1 < len(dataset) :\n",
    "\n",
    "            diff=q50_predict-dataset[j+1]\n",
    "            print('data',dataset[j+1],'diff',diff)\n",
    "            anomalies.append(diff)\n",
    "            comparison_dataset.append(dataset[j+1])\n",
    "        j = j + 1\n",
    "        i = i + 1\n",
    "\n",
    "    anomalies_array=[]\n",
    "\n",
    "    for h in range(len(anomalies)):\n",
    "        internal = anomalies[h]\n",
    "        internal_array = []\n",
    "        anomalies_array.append(internal[0])\n",
    "    anomalies_array = scaler.inverse_transform(anomalies_array)\n",
    "    comparison_dataset=scaler.inverse_transform(comparison_dataset)\n",
    "    print(anomalies_array)\n",
    "    for itr in range(len(anomalies_array)):\n",
    "        print('data',comparison_dataset[itr],'diff',anomalies_array[itr])\n",
    "\n",
    "    anomalies = identify_anomaly(anomalies_array)\n",
    "    print(anomalies)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
