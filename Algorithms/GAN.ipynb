{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J74hp8COJGC-"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.8.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_x6RDk7HW_6"
      },
      "outputs": [],
      "source": [
        "#Import data manipulation libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "#Importing ML/DL libraries\n",
        "from sklearn.metrics import roc_curve, auc,precision_recall_fscore_support, auc\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import Input, Dense, Activation\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "df = pd.read_csv('yahoo3.csv')\n",
        "dimension=len(df.columns)\n",
        "\n",
        "def get_generator(optimizer):\n",
        "    generator = Sequential()\n",
        "    generator.add(Dense(64, input_dim=dimension))\n",
        "    generator.add(Activation('tanh'))\n",
        "\n",
        "    generator.add(Dense(128))\n",
        "    generator.add(Activation('tanh'))\n",
        "\n",
        "    generator.add(Dense(256))\n",
        "    generator.add(Activation('tanh'))\n",
        "\n",
        "    generator.add(Dense(256))\n",
        "    generator.add(Activation('tanh'))\n",
        "\n",
        "    generator.add(Dense(512))\n",
        "    generator.add(Activation('tanh'))\n",
        "\n",
        "    generator.add(Dense(dimension, activation='tanh'))\n",
        "\n",
        "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    return generator\n",
        "\n",
        "\n",
        "def get_discriminator(optimizer):\n",
        "    discriminator = Sequential()\n",
        "    discriminator.add(Dense(256, input_dim=dimension))\n",
        "    discriminator.add(Activation('relu'))\n",
        "    discriminator.add(Dropout(0.2))\n",
        "\n",
        "    discriminator.add(Dense(128))\n",
        "    discriminator.add(Activation('relu'))\n",
        "    discriminator.add(Dropout(0.2))\n",
        "\n",
        "    discriminator.add(Dense(128))\n",
        "    discriminator.add(Activation('relu'))\n",
        "    discriminator.add(Dropout(0.2))\n",
        "\n",
        "    discriminator.add(Dense(128))\n",
        "    discriminator.add(Activation('relu'))\n",
        "    discriminator.add(Dropout(0.2))\n",
        "\n",
        "    discriminator.add(Dense(128))\n",
        "    discriminator.add(Activation('relu'))\n",
        "    discriminator.add(Dropout(0.2))\n",
        "\n",
        "    discriminator.add(Dense(1))\n",
        "    discriminator.add(Activation('sigmoid'))\n",
        "\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    return discriminator\n",
        "\n",
        "\n",
        "def get_gan_network(discriminator, generator, optimizer, input_dim=dimension):\n",
        "    discriminator.trainable = False\n",
        "    gan_input = Input(shape=(input_dim,))\n",
        "    x = generator(gan_input)\n",
        "    gan_output = discriminator(x)\n",
        "\n",
        "    gan = Model(inputs=gan_input, outputs=gan_output)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    return gan\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    learning_rate = 0.00001\n",
        "    batch_size = 150\n",
        "    epochs = 50\n",
        "    adam = Adam(lr=learning_rate, beta_1=0.5)\n",
        "    actual = pd.read_csv('yahoo3.csv', usecols=['Class'])\n",
        "    y_true = actual\n",
        "    dataset = pd.read_csv('yahoo3.csv',usecols=lambda column: column != 'Class')\n",
        "    tmp=len(dataset)\n",
        "    dimension=len(dataset.columns)\n",
        "    dataset = dataset.astype('float32')\n",
        "    dataset = np.array(dataset)\n",
        "    normal_data = dataset[actual['Class'] == 0]\n",
        "    train_size = int(tmp * 0.7)\n",
        "    test_size = tmp\n",
        "    train_data=[]\n",
        "    for i in range(train_size):\n",
        "        train_data.append(normal_data[i])\n",
        "    test_data=dataset\n",
        "    train_data = np.array(train_data)\n",
        "    batch_count = train_data.shape[0] // batch_size\n",
        "    pbar = tqdm(total=epochs * batch_count)\n",
        "    gan_loss = []\n",
        "    discriminator_loss = []\n",
        "\n",
        "    # Inititalizing the network\n",
        "    generator = get_generator(adam)\n",
        "    discriminator = get_discriminator(adam)\n",
        "    gan = get_gan_network(discriminator, generator, adam, input_dim=dimension)\n",
        "\n",
        "    # Print initial weights of the generator\n",
        "    print(\"Initial Weights of Generator:\")\n",
        "    for layer in generator.layers:\n",
        "        if hasattr(layer, 'get_weights'):\n",
        "            weights = layer.get_weights()\n",
        "            if weights:\n",
        "                print(f\"{layer.name} Weights: {weights}\")\n",
        "\n",
        "    # Print initial weights of the discriminator\n",
        "    print(\"\\nInitial Weights of Discriminator:\")\n",
        "    for layer in discriminator.layers:\n",
        "        if hasattr(layer, 'get_weights'):\n",
        "            weights = layer.get_weights()\n",
        "            if weights:\n",
        "                print(f\"{layer.name} Weights: {weights}\")\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for index in range(batch_count):\n",
        "            pbar.update(1)\n",
        "            # Creating a random set of input noise and images\n",
        "            noise = np.random.normal(0, 1, size=[batch_size, dimension])\n",
        "\n",
        "            # Generate fake samples\n",
        "            generated_images = generator.predict_on_batch(noise)\n",
        "\n",
        "            # Obtain a batch of normal network packets\n",
        "            image_batch = train_data[index * batch_size: (index + 1) * batch_size]\n",
        "\n",
        "            X = np.vstack((generated_images, image_batch))\n",
        "            y_dis = np.ones(2 * batch_size)\n",
        "            y_dis[:batch_size] = 0\n",
        "\n",
        "            # Train discriminator\n",
        "            discriminator.trainable = True\n",
        "            d_loss = discriminator.train_on_batch(X, y_dis)\n",
        "\n",
        "            # Train generator\n",
        "            noise = np.random.uniform(0, 1, size=[batch_size, dimension])\n",
        "            y_gen = np.ones(batch_size)\n",
        "            discriminator.trainable = False\n",
        "            g_loss = gan.train_on_batch(noise, y_gen)\n",
        "\n",
        "            # Record the losses\n",
        "            discriminator_loss.append(d_loss)\n",
        "            gan_loss.append(g_loss)\n",
        "\n",
        "    dataframe1 = pd.read_csv('yahoo3.csv', usecols=['Class'], engine='python')\n",
        "    dataset1 =pd.read_csv('yahoo3.csv',usecols=lambda column: column != 'Class')\n",
        "    test_x_predictions = discriminator.predict(dataset1)\n",
        "    y_pred = np.array(test_x_predictions)\n",
        "    per = np.percentile(test_x_predictions, 10)\n",
        "    inds = (y_pred > per)\n",
        "\n",
        "    inds_comp = (y_pred <= per)\n",
        "    y_pred[inds] = 0\n",
        "    y_pred[inds_comp] = 1\n",
        "    precision_gan, recall_gan, f1_score_gan, _ = precision_recall_fscore_support(actual, y_pred, average='binary')\n",
        "\n",
        "    # Calculate ROC curve\n",
        "    fpr6, tpr6, _ = roc_curve(actual, y_pred)\n",
        "\n",
        "    # Calculate AUC (Area Under the ROC Curve)\n",
        "    auc_roc_gan = auc(fpr6, tpr6)\n",
        "\n",
        "    print ('Precision :',precision_gan ),\n",
        "    print ('Recall :',recall_gan ),\n",
        "    print ('F1 :',f1_score_gan )\n",
        "    print('AUC ROC: ',auc_roc_gan)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP5226w3Eczv62f9/DwzB6b",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
