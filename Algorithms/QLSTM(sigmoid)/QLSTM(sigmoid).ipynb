{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgtLAd4dcCF3nH5sfFr/f3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apbxoSGN0ZR5"
      },
      "outputs": [],
      "source": [
        "filename='yahoo2.csv'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import read_csv\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score, roc_curve, auc\n",
        "\n",
        "period_size = 234\n",
        "step_size = 117\n",
        "upper_q_threshold=0.9999\n",
        "lower_q_threshold=0.0001\n",
        "anomaly_res =[-563.9761436,-2416.661669,-3232.353099,-3565.527389,-2560.50266,-4133.645629,-4004.058979,-4279.506572,-4804.726653,-5948.068253]\n",
        "\n",
        "def intersection(lst1, lst2):\n",
        "    lst3=[]\n",
        "    for itr in range(len(lst1)):\n",
        "        if lst1[itr][0] in lst2:\n",
        "            lst3.append(lst1[itr])\n",
        "    return lst3\n",
        "\n",
        "\n",
        "\n",
        "def verify_stationarity(dataset):\n",
        "    is_stationary=True\n",
        "    test_results = adfuller(dataset)\n",
        "\n",
        "    print(f\"ADF test statistic: {test_results[0]}\")\n",
        "    print(f\"p-value: {test_results[1]}\")\n",
        "    print(\"Critical thresholds:\")\n",
        "\n",
        "    for key, value in test_results[4].items():\n",
        "        print(f\"\\t{key}: {value}\")\n",
        "    itr = 0\n",
        "    for key, value in test_results[4].items():\n",
        "       print('\\t%s: %.3f' % (key, value))\n",
        "       if itr==0:\n",
        "         critical=value\n",
        "       itr=itr+1\n",
        "\n",
        "    print('critical',critical)\n",
        "    if test_results[0] > critical:\n",
        "         print('non stationary')\n",
        "         is_stationary=False\n",
        "    return  is_stationary\n",
        "\n",
        "def create_dataset(dataset, look_back=1, tw=3):\n",
        "    dataX, dataY = [], []  # dtaset for mean\n",
        "    datastdX, datastdY = [], []  # dataset for std\n",
        "    datacombX, datacomY = [], []  # dataset for mean and std for third deep learning\n",
        "    multi = look_back // tw\n",
        "    for i in range(len(dataset) - look_back - 1):\n",
        "        q10X = []\n",
        "        q90X = []\n",
        "        a = dataset[i + 1:(i + look_back + 1)]\n",
        "        indices = i + (multi - 1) * tw\n",
        "        # print('last window', dataset[indices:(i + look_back), 0])\n",
        "        b = numpy.quantile(a, lower_q_threshold)\n",
        "        c = numpy.quantile(a, upper_q_threshold)\n",
        "        for j in range(0, len(a), tw):\n",
        "            q10 = numpy.quantile(a[j:j + tw], lower_q_threshold)\n",
        "            q90 = numpy.quantile(a[j:j + tw], upper_q_threshold)\n",
        "            q10X.append(q10)\n",
        "            q90X.append(q90)\n",
        "        dataX.append(q10X)\n",
        "        datastdX.append(q90X)\n",
        "        dataY.append(b)\n",
        "        datastdY.append(c)\n",
        "        comb = []\n",
        "        comb.append(b)\n",
        "        comb.append(c)\n",
        "        datacombX.append(comb)\n",
        "        datacomY.append(dataset[i + look_back, 0])\n",
        "    return numpy.array(dataX), numpy.array(dataY), numpy.array(datastdX), numpy.array(datastdY), numpy.array(\n",
        "        datacombX), numpy.array(datacomY)\n",
        "\n",
        "\n",
        "def identify_anomaly(finalres_q10,finalres_q90,ts):\n",
        "    ''' if ( actual_values[i] > avg_q90) or (\n",
        "                         actual_values[i] < avg_q10):'''\n",
        "    anomaly_detection = []\n",
        "    for m in range(0, len(finalres_q10), period_size):\n",
        "        actual_values = ts[m:m + period_size]\n",
        "        avg_q10 = numpy.average(finalres_q10[m:m + period_size])\n",
        "        avg_q90 = numpy.average(finalres_q90[m:m + period_size])\n",
        "\n",
        "\n",
        "        for i in range(len(actual_values)):\n",
        "            if (actual_values[i] > avg_q90) or (\n",
        "                    actual_values[i] < avg_q10):\n",
        "                anomaly_detection.append(actual_values[i])\n",
        "\n",
        "\n",
        "\n",
        "    return anomaly_detection\n",
        "\n",
        "def find_anomalies(lowest_leaf):\n",
        "    ts = read_csv(filename, usecols=lambda column: column != 'Class').values\n",
        "\n",
        "    y_pred = numpy.zeros(len(ts), dtype=int)\n",
        "\n",
        "    lowest_leaf = numpy.array(lowest_leaf)\n",
        "\n",
        "    for lowest_leaf_array in lowest_leaf:\n",
        "        diffs = numpy.abs(ts - lowest_leaf_array)\n",
        "        max_diffs = numpy.max(diffs, axis=1)\n",
        "        outliers = max_diffs <= 1e-4\n",
        "        y_pred[outliers] = 1\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # fix random seed for reproducibility\n",
        "    numpy.random.seed(7)\n",
        "    dataframe = read_csv(filename)\n",
        "    y_true=dataframe.iloc[:, -1]\n",
        "    label_0_rows = dataframe[dataframe['Class'] == 0]\n",
        "    percent_to_select = 0.7\n",
        "    num_to_select = int(len(label_0_rows) * percent_to_select)\n",
        "    selected_rows = label_0_rows.sample(n=num_to_select, random_state=42)\n",
        "    selected_rows = selected_rows['Value'] # put column name corresponding to the name of the column containing the datapoints in the dataset\n",
        "    dataset = selected_rows.values\n",
        "    dataset = dataset.reshape(-1,1)\n",
        "    stationary=verify_stationarity(dataset)\n",
        "    dataset = dataset.astype('float32')\n",
        "    # normalize the dataset\n",
        "    stationary = verify_stationarity(dataset)\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "    dataset = scaler.fit_transform(dataset)\n",
        "    # split into train and test sets\n",
        "    train_size = int(len(dataset) * 0.7)\n",
        "    test_size = len(dataset) - train_size\n",
        "    train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\n",
        "    # reshape into X=t and Y=t+1\n",
        "    look_back = period_size\n",
        "    tw = step_size\n",
        "    multi = look_back // tw\n",
        "    trainX, trainY, trainstdX, trainstdY, traincombX, traincombY = create_dataset(train, look_back, tw)\n",
        "    testX, testY, teststdX, teststdY, testcombX, testcombY = create_dataset(test, look_back, tw)\n",
        "    # reshape input to be [samples, time steps, features]\n",
        "    trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
        "    testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
        "    trainstdX = numpy.reshape(trainstdX, (trainstdX.shape[0], trainstdX.shape[1], 1))\n",
        "    teststdX = numpy.reshape(teststdX, (teststdX.shape[0], teststdX.shape[1], 1))\n",
        "    traincombX = numpy.reshape(traincombX, (traincombX.shape[0], traincombX.shape[1], 1))\n",
        "    testcombX = numpy.reshape(testcombX, (testcombX.shape[0], testcombX.shape[1], 1))\n",
        "    modelq10 = Sequential()\n",
        "    modelq10.add(LSTM(4, input_shape=(multi, 1), activation='sigmoid', recurrent_activation='sigmoid'))\n",
        "    modelq10.add(Dense(1))\n",
        "    modelq10.compile(loss='log_cosh', optimizer='adam')\n",
        "    modelq10.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
        "    modelq90 = Sequential()\n",
        "    modelq90.add(LSTM(4, input_shape=(multi, 1), activation='sigmoid', recurrent_activation='sigmoid'))\n",
        "    modelq90.add(Dense(1))\n",
        "    modelq90.compile(loss='log_cosh', optimizer='adam')\n",
        "    modelq90.fit(trainstdX, trainstdY, epochs=100, batch_size=1, verbose=2)\n",
        "\n",
        "    i = 0\n",
        "    j = look_back\n",
        "    actual_quantile_interval = []\n",
        "    steps = tw\n",
        "    positive = True\n",
        "    anomalies=[]\n",
        "    finalres_q10 = []\n",
        "    finalres_q90 = []\n",
        "    dataframe = read_csv(filename, usecols=[0], engine='python')\n",
        "    dataset = dataframe.values\n",
        "    dataset = scaler.fit_transform(dataset)\n",
        "    ts = dataset\n",
        "    ts_accumulate=[]\n",
        "    while j <= len(dataset):\n",
        "        q10_array = []\n",
        "        q90_array = []\n",
        "\n",
        "        temp = dataset[i:j]\n",
        "        actual_quantile_interval.append(\n",
        "            numpy.absolute(numpy.quantile(dataset[i + 1:j + 1], lower_q_threshold) - numpy.quantile(dataset[i + 1:j + 1], upper_q_threshold)))\n",
        "\n",
        "        for m in range(0, len(temp), steps):\n",
        "            q10array = []\n",
        "            q90array = []\n",
        "            q10 = numpy.quantile(temp[m:m + steps], lower_q_threshold)\n",
        "            q90 = numpy.quantile(temp[m:m + steps], upper_q_threshold)\n",
        "            q10array.append(q10)\n",
        "            q90array.append(q90)\n",
        "            q90_array.append(q90array)\n",
        "            q10_array.append(q10array)\n",
        "\n",
        "        final_q10_array = []\n",
        "        final_q10_array.append(q10_array)\n",
        "        print('final_q10_array', final_q10_array)\n",
        "        q10_predict = modelq10.predict(final_q10_array)\n",
        "        print('q10 predict', q10_predict)\n",
        "        final_q90_array = []\n",
        "        final_q90_array.append(q90_array)\n",
        "        print('final_q90_array', final_q90_array)\n",
        "        final_q90_array = numpy.array(final_q90_array)\n",
        "        q90_predict = modelq90.predict(final_q90_array)\n",
        "        print('predict', q90_predict)\n",
        "        if j+1 < len(dataset) and (dataset[j+1]> q90_predict or dataset[j+1]<q10_predict):\n",
        "            anomalies.append(dataset[j+1])\n",
        "            dataset=numpy.delete(dataset,j+1)\n",
        "            print('length',len(dataset))\n",
        "        finalres_q10.append(q10_predict)\n",
        "        finalres_q90.append(q90_predict)\n",
        "        if j+1 < len(dataset):\n",
        "           ts_accumulate.append(dataset[j+1])\n",
        "        j = j + 1\n",
        "        i = i + 1\n",
        "\n",
        "    prediction_array_q10 = []\n",
        "    prediction_array_q90 = []\n",
        "    anomalies_array=[]\n",
        "\n",
        "    for h in range(len(anomalies)):\n",
        "        internal = anomalies[h]\n",
        "        internal_array = []\n",
        "        internal_array.append(internal)\n",
        "        anomalies_array.append(internal_array)\n",
        "    anomalies_array = scaler.inverse_transform(anomalies_array)\n",
        "    '''print(anomalies_array)\n",
        "    print('anomaly length',len(anomalies_array))'''\n",
        "    ts_accumulate_another=[]\n",
        "    for h in range(len(finalres_q10)):\n",
        "        internal = finalres_q10[h]\n",
        "        internal_q90 = finalres_q90[h]\n",
        "        prediction_array_q10.append(internal[0])\n",
        "        prediction_array_q90.append(internal_q90[0])\n",
        "    for g in range(len(ts_accumulate)):\n",
        "        internal=[]\n",
        "        internal.append(ts_accumulate[g])\n",
        "        ts_accumulate_another.append(internal)\n",
        "    finalres_q10 = scaler.inverse_transform(prediction_array_q10)\n",
        "    finalres_q90 = scaler.inverse_transform(prediction_array_q90)\n",
        "    '''trunc_finalres = []\n",
        "    for g in range(len(finalres)):\n",
        "        trunc_finalres.append(finalres[g])'''\n",
        "    ts = ts[look_back:]\n",
        "    ts = scaler.inverse_transform(ts)\n",
        "    ts_accumulate=scaler.inverse_transform(ts_accumulate_another)\n",
        "    print('lenght', len(ts_accumulate), 'actual_quantile_interval', len(finalres_q10))\n",
        "    '''ts_array = []\n",
        "    for g in range(len(ts)):\n",
        "        ts_array.append(ts[g])'''\n",
        "    finalres_q10_array=[]\n",
        "    finalres_q90_array=[]\n",
        "    for g in range(len(finalres_q10)-1):\n",
        "        finalres_q10_array.append(finalres_q10[g])\n",
        "        finalres_q90_array.append(finalres_q90[g])\n",
        "    prediction_errors = []\n",
        "    anomalies = identify_anomaly(finalres_q10_array,finalres_q90_array,ts)\n",
        "    y_pred = find_anomalies(anomalies)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1_score = (2*(precision*recall))/(precision + recall)\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
        "    auc_roc = auc(fpr, tpr)\n",
        "\n",
        "    print(\"Precision:\")\n",
        "    print(precision)\n",
        "    print(\"Recall:\")\n",
        "    print(recall)\n",
        "    print(\"F1 Score:\")\n",
        "    print(f1_score)\n",
        "    print(\"AUC ROC:\")\n",
        "    print(auc_roc)"
      ],
      "metadata": {
        "id": "Jzs1psND0d2a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}