{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLoOrEeoPuyQ4QA8vwUXPu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install kneed"
      ],
      "metadata": {
        "id": "otIUswKRjlHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "140e1V8KjQIT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, roc_auc_score, roc_curve, auc\n",
        "from sklearn.cluster import KMeans\n",
        "import math\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
        "import warnings\n",
        "from kneed import KneeLocator\n",
        "\n",
        "\n",
        "# Filter out the specific warning by category and message\n",
        "warnings.filterwarnings(\"ignore\", message=\"The default value of `n_init` will change from 10 to 'auto' in 1.4.*\")\n",
        "\n",
        "\n",
        "leaf_nodes=[]\n",
        "child_tree = []\n",
        "\n",
        "def binary_tree(result,final_cluster, count,tree_dic):\n",
        "    first_cluster = []\n",
        "    second_cluster = []\n",
        "    if count==0:\n",
        "        tree_dic[count]=result\n",
        "    else:\n",
        "        key=max(tree_dic)+1\n",
        "        tree_dic[key] = result\n",
        "    kmeans = KMeans(n_clusters=2, random_state=0).fit(result)\n",
        "\n",
        "    score=0.1\n",
        "\n",
        "    if score < 0.9 or count==0:\n",
        "        for itr in range(len(result)):\n",
        "            if final_cluster[itr] == 1:\n",
        "                first_cluster.append(result[itr])\n",
        "            else:\n",
        "                second_cluster.append(result[itr])\n",
        "\n",
        "\n",
        "        count = count + 1\n",
        "\n",
        "        if len(first_cluster)> minimum_cluster_threshold :\n",
        "           kmeans = KMeans(n_clusters=2, random_state=0).fit(first_cluster)\n",
        "\n",
        "\n",
        "\n",
        "           binary_tree(first_cluster,kmeans.labels_,count,tree_dic)\n",
        "        else:\n",
        "            key = max(tree_dic) + 1\n",
        "            tree_dic[key]=first_cluster\n",
        "            leaf_nodes.append(key)\n",
        "\n",
        "            if count <= leaf_level_threshold:\n",
        "\n",
        "                child_tree.extend(first_cluster)\n",
        "\n",
        "\n",
        "        if len(second_cluster) >minimum_cluster_threshold:\n",
        "            kmeans = KMeans(n_clusters=2, random_state=0).fit(second_cluster)\n",
        "\n",
        "\n",
        "            binary_tree(second_cluster,kmeans.labels_,count,tree_dic)\n",
        "        else:\n",
        "            key = max(tree_dic) + 1\n",
        "            tree_dic[key] = second_cluster\n",
        "            leaf_nodes.append(key)\n",
        "\n",
        "            if count <= leaf_level_threshold:\n",
        "                child_tree.extend(second_cluster)\n",
        "\n",
        "\n",
        "    else:\n",
        "        key = max(tree_dic)\n",
        "        leaf_nodes.append(key)\n",
        "\n",
        "\n",
        "\n",
        "def get_large_cluster_centroid(small_cluster_threshold,tree_dic):\n",
        "    merged_clusters = {}\n",
        "    mergeditr = 0\n",
        "    for key in tree_dic:\n",
        "        if  key in leaf_nodes :\n",
        "            if len(tree_dic[key]) > small_cluster_threshold:\n",
        "               merged_clusters[mergeditr] = tree_dic[key]\n",
        "               mergeditr = mergeditr + 1\n",
        "    final_array = []\n",
        "    count = 0\n",
        "    for key in merged_clusters:\n",
        "        for itr in range(len(merged_clusters[key])):\n",
        "            array = merged_clusters[key]\n",
        "            final_array.insert(count, array[itr])\n",
        "            count = count + 1\n",
        "    '''merged_clustersarray=util.convert_dict_to_list(merged_clusters,len(merged_clusters),len(tree_dic[0]))'''\n",
        "    centroid = np.mean(final_array, axis=0)\n",
        "    return centroid\n",
        "\n",
        "def get_large_cluster_center(small_cluster_threshold,tree_dic):\n",
        "    merged_clusters = {}\n",
        "    centroid_array=[]\n",
        "    mergeditr = 0\n",
        "    for key in tree_dic:\n",
        "        if key in leaf_nodes:\n",
        "            if len(tree_dic[key]) > small_cluster_threshold:\n",
        "                merged_clusters[mergeditr] = tree_dic[key]\n",
        "                mergeditr = mergeditr + 1\n",
        "\n",
        "    count = 0\n",
        "    for key in merged_clusters:\n",
        "        centroid = np.mean(merged_clusters[key], axis=0)\n",
        "        centroid_array.insert(count,centroid)\n",
        "        count = count + 1\n",
        "    '''this is returning the cluster centers of all the large clusters'''\n",
        "\n",
        "    return centroid_array\n",
        "\n",
        "def get_large_clusters_anomalyscore(tree_dic,small_cluster_threshold):\n",
        "    large_clusters={}\n",
        "    anomaly_largeclusters={}\n",
        "    for key in tree_dic:\n",
        "        if key in leaf_nodes:\n",
        "           if len(tree_dic[key])>=small_cluster_threshold:\n",
        "               large_clusters[key]=tree_dic[key]\n",
        "\n",
        "    for key in large_clusters:\n",
        "        array = large_clusters[key]\n",
        "        centroid = np.mean(array, axis=0)\n",
        "        for itr in range(len(large_clusters[key])):\n",
        "            anomalyscore_largeclusters=np.linalg.norm(array[itr]-centroid)\n",
        "            anomaly_largeclusters[anomalyscore_largeclusters]=array[itr]\n",
        "\n",
        "    return anomaly_largeclusters\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    ts = pd.read_csv(filename)\n",
        "    ts=np.array(ts)\n",
        "    st_ts = ts.tolist()\n",
        "    st_ts = sorted(st_ts)\n",
        "    st_ts = np.array(st_ts)\n",
        "    actual = st_ts[:, -1]\n",
        "\n",
        "    ts = pd.read_csv(filename, usecols=lambda column: column != 'Class')\n",
        "    ts=np.array(ts)\n",
        "    print(ts.shape)\n",
        "    count = 0\n",
        "    tree_dic = {}\n",
        "    data_size=len(ts)\n",
        "    kmeans = KMeans(n_clusters=2, random_state=0).fit(ts)\n",
        "    small_cluster_threshold = math.floor(.02 * len(ts))\n",
        "    leaf_level_threshold=3\n",
        "    minimum_cluster_threshold = math.floor(.1 * len(ts))\n",
        "\n",
        "    final_cluster = kmeans.labels_\n",
        "    binary_tree(ts, final_cluster, count, tree_dic)\n",
        "    minkey = math.inf\n",
        "    total_cluster_length = 0\n",
        "    #total cluster length of the leaf clusters together,cal min key in the leaf cluster\n",
        "    for key in tree_dic:\n",
        "        if key in leaf_nodes:\n",
        "            total_cluster_length = total_cluster_length + len(tree_dic[key])\n",
        "            if key < minkey:\n",
        "               minkey=key\n",
        "\n",
        "    #identifying small clusters in the data and merging them into a single list (final_array). It then calculates the centroid of these merged data points,\n",
        "    anomalyscore_dict={}\n",
        "    anomalyscore_from_largeclusters={}\n",
        "    merged_clusters={}\n",
        "    mergeditr=0\n",
        "    for key in tree_dic:\n",
        "        if len(tree_dic[key])<small_cluster_threshold:\n",
        "            merged_clusters[mergeditr]=tree_dic[key]\n",
        "            mergeditr=mergeditr+1\n",
        "    final_array=[]\n",
        "    count=0\n",
        "    for key in merged_clusters:\n",
        "        for itr in range(len(merged_clusters[key])):\n",
        "            array=merged_clusters[key]\n",
        "            final_array.insert(count,array[itr])\n",
        "            count=count+1\n",
        "    '''merged_clustersarray=util.convert_dict_to_list(merged_clusters,len(merged_clusters),len(tree_dic[0]))'''\n",
        "    centroid = np.mean(final_array, axis=0)\n",
        "\n",
        "\n",
        "    #anomaly score by subtracting value from centroid\n",
        "    for anoitr in range(len(final_array)):\n",
        "        anomalyscore=np.linalg.norm(final_array[anoitr]-centroid)\n",
        "        anomalyscore_dict[anomalyscore]=final_array[anoitr]\n",
        "\n",
        "    normal_cluster_cent=get_large_cluster_centroid(small_cluster_threshold,tree_dic)\n",
        "    for anoitr in range(len(final_array)):\n",
        "        anomalyscore_largeclusters=np.linalg.norm(final_array[anoitr]-normal_cluster_cent)\n",
        "        anomalyscore_from_largeclusters[anomalyscore_largeclusters]=final_array[anoitr]\n",
        "    centroid_array=get_large_cluster_center(small_cluster_threshold,tree_dic)\n",
        "    # '''get cblof : cluster based local outlier factor.. which calculates the min distance of the data points belong to the smallest clusters to the largest clusters centriod'''\n",
        "    cblof={}\n",
        "    for anoitr in range(len(final_array)):\n",
        "        min=math.inf\n",
        "        for clusitr in range(len(centroid_array)):\n",
        "\n",
        "         cblof_dist=np.linalg.norm(final_array[anoitr]-centroid_array[clusitr])\n",
        "         if cblof_dist<min:\n",
        "             min=cblof_dist\n",
        "         if  clusitr==len(centroid_array)-1:\n",
        "             cblof[min] = final_array[anoitr]\n",
        "\n",
        "    cblof_largecluster=get_large_clusters_anomalyscore(tree_dic,small_cluster_threshold)\n",
        "    final_merged_cblof={**cblof,**cblof_largecluster}\n",
        "    anomaly_scores_list = list(final_merged_cblof.keys())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    sorted_scores = np.sort(anomaly_scores_list)\n",
        "    cumulative_sum = np.cumsum(sorted_scores)\n",
        "    percentage_sum = (cumulative_sum / cumulative_sum[-1]) * 100\n",
        "\n",
        "    knee = KneeLocator(range(1, len(percentage_sum) + 1), percentage_sum, curve=\"convex\", direction=\"increasing\")\n",
        "    knee_threshold = sorted_scores[knee.elbow]\n",
        "\n",
        "    y_anom = [data_point for anomaly_score, data_point in final_merged_cblof.items() if anomaly_score > knee_threshold]\n",
        "\n",
        "    anomaly_score=[]\n",
        "\n",
        "\n",
        "    tmp=[]\n",
        "\n",
        "    count=0\n",
        "\n",
        "\n",
        "    y_anom = np.vstack(y_anom)\n",
        "    x_indices = np.arange(len(tmp))\n",
        "    st_ts = ts.tolist()\n",
        "    st_ts = sorted(st_ts)\n",
        "    st_ts = np.array(st_ts)\n",
        "    st_ft = y_anom.tolist()\n",
        "    st_ft = sorted(st_ft)\n",
        "    st_ft = np.array(st_ft)\n",
        "    indices=[]\n",
        "    count=0\n",
        "    for i, element in enumerate(st_ts):\n",
        "      if ((count<=(len(st_ft)-1)) and np.allclose(st_ft[count], element,atol=1e-04)):\n",
        "          indices.append(i)\n",
        "          while i + 1 < len(st_ts) and np.allclose(st_ts[i], st_ts[i + 1],atol=1e-04):\n",
        "              indices.append(i + 1)\n",
        "              i += 1\n",
        "          count += 1\n",
        "\n",
        "    print(count)\n",
        "\n",
        "    y_pred_test = np.zeros(len(ts), dtype=int)\n",
        "\n",
        "    y_pred_test[indices]=1\n",
        "    print(y_pred_test)\n",
        "    fpr10, tpr10, thresholds = roc_curve(actual, y_pred_test)\n",
        "    auc_roc_db = auc(fpr10, tpr10)\n",
        "    recall_db = recall_score(actual, y_pred_test)\n",
        "    precision_db = precision_score(actual, y_pred_test)\n",
        "    f1_score_db = f1_score(actual, y_pred_test)\n",
        "\n",
        "    print()\n",
        "    print(\" Precision:{:.4f}\".format(precision_db))\n",
        "    print(\" Recall:{:.4f}\".format(recall_db))\n",
        "    print(\" F1-score:{:.4f}\".format(f1_score_db))\n",
        "    print(\" AUC-ROC:{:.4f}\".format(auc_roc_db))\n",
        "\n",
        "\n"
      ]
    }
  ]
}